{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b02dc835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raster projected seem to be OK!\n",
      "Contour shapefile is created ... OK!\n",
      "Slope raster is created ... OK!\n",
      "Buffered outline is created ... OK!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4643/673255546.py:284: RuntimeWarning: Mean of empty slice\n",
      "  meanRasterSlope = np.nanmean(maskedRaster)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling is completed ... OK!\n",
      "Interpolation is completed ... OK!\n",
      "Thickness raster is created ... OK!\n",
      "Bed topograpgy is created ... OK!\n",
      "\n",
      "Total time elapsed =  1.9378161150089  seconds\n"
     ]
    }
   ],
   "source": [
    "# WARNING - HIGHLY EXPERIMENTAL !! USE AT YOUR OWN RISK.\n",
    "\n",
    "# ======================================================================================================================\n",
    "# GLAcier Bed EsTimation by varying basal stress. [GLABET]\n",
    "# ======================================================================================================================\n",
    "\n",
    "\n",
    "# ======================================================================================================================\n",
    "\n",
    "#                           Author:\n",
    "#                        Saugat Paudel\n",
    "#       M.S. by Research in Glaciology, Kathmandu University, Nepal.\n",
    "#              Model created as a part of thesis requirement\n",
    "#                   Email: saugat.email@gmail.com\n",
    "\n",
    "# ======================================================================================================================\n",
    "import traceback\n",
    "import time,gc\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from shapely.ops import  unary_union\n",
    "from shapely.geometry import shape, mapping, LineString, MultiPolygon, Polygon\n",
    "import fiona\n",
    "import subprocess\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "from rasterio.mask import mask\n",
    "from rasterio.crs import CRS\n",
    "# ======================================================================================================================\n",
    "__author__ = 'Saugat Paudel'\n",
    "__version__ = '1.2'\n",
    "__email__ = 'saugat.email@gmail.com'\n",
    "__date__ = '6 March 2017'\n",
    "# ======================================================================================================================\n",
    "# 09026_1,   #49367_1.mat,   #53222_1.mat,    #53249_1.mat,    #54211_1.mat,     #54364_1.mat\n",
    "\n",
    "# Full Path to the glacier outline shapefile.\n",
    "glacierOutlineFullPath = '/mnt/d/area/glims/glenglat_asia/per_glacier/reproject_shp/49367.shp'\n",
    "\n",
    "\n",
    "# Full Path to the glacier DEM file. The DEM should preferably have same extent as the outline.\n",
    "demFullPath = '/mnt/d/area/glims/glenglat_asia/per_glacier/reproject_thk_dem/49367_dem.tif'\n",
    "\n",
    "# Full path to the desired output folder.\n",
    "outputFolder =  '/mnt/d/area/glims/glenglat_asia/per_glacier/glenglat_dem'\n",
    "# WARNING - HIGHLY EXPERIMENTAL !! USE AT YOUR OWN RISK.\n",
    "\n",
    "# Parameters\n",
    "density = 917  # In kilograms per cubic meters. Default: 900\n",
    "f = 0.8  # Dimensionless. Default: 0.8\n",
    "g = 9.81  # Acceleration due to gravity in meters per seconds squared. Default: 9.81\n",
    "\n",
    "# ======================================================================================================================\n",
    "# ======================================================================================================================\n",
    "\n",
    "\n",
    "# ======================================================================================================================\n",
    "# Other variables. Not necessary to change.\n",
    "# ======================================================================================================================\n",
    "\n",
    "# Path to 'nnbathy' interpolation algorithm\n",
    "'''\n",
    "The nnbathy algorithm is included within the model folder. See glabet/nnbathy. In windows it may be necessary \n",
    "to change the slashes in the folder structure. Also it may be necessary to put '.exe' at the end. This depends on how \n",
    "the nnbathy algorithm was compiled.\n",
    "\n",
    "Look in the 'nnbathy' help/readme to find out more about compilation\n",
    "'''\n",
    "\n",
    "# Give the relative path to the 'nnbathy' interpolation algorithm here. Edit 'nnbathy/nnbathy' part.\n",
    "nnPATH = '/home/slw/software/nn-c/nn/nnbathy'\n",
    "\n",
    "\n",
    "# Output File Paths\n",
    "basename=os.path.splitext(os.path.basename(glacierOutlineFullPath))[0]\n",
    "contourFullPath = os.path.join(outputFolder, basename+'_contour.shp')\n",
    "slopeFullPath = os.path.join(outputFolder, basename+'_slope.tif')\n",
    "sampledCSV = os.path.join(outputFolder, basename+'_ungriddedCSV.txt')\n",
    "finalCSV = os.path.join(outputFolder, basename+'_finalCSV.txt')\n",
    "thicknessRaster = os.path.join(outputFolder, basename+'_Thickness.tif')\n",
    "bedTopoFullPath = os.path.join(outputFolder, basename+'_BedTopo.tif')\n",
    "bufferedOutlineFullPath = os.path.join(outputFolder, basename+'_bufferedOutline.shp')\n",
    "\n",
    "# ======================================================================================================================\n",
    "# PARAMETERS TO CHANGE.\n",
    "# ======================================================================================================================\n",
    "\n",
    "# Paths  09026.shp,   49367.shp,   53222.shp,    53249.shp,    54211.shp,     54364.shp\n",
    "\n",
    "\n",
    "# Check for the necessary files. This does not check the existence of the output folder.\n",
    "def checkFiles():\n",
    "    if os.path.exists(glacierOutlineFullPath) is False:\n",
    "        print('Glacier outline not found!! Model will now exit.')\n",
    "        exit()\n",
    "    elif os.path.exists(demFullPath) is False:\n",
    "        print('DEM not found!! Model will now exit.')\n",
    "        exit()\n",
    "    else:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# Create contour from the given DEM. For this function to work 'gdal' libraries needs to be installed.\n",
    "def createContour(pathToDEMFile, outputPath, contourInterval):\n",
    "    subprocess.call(\n",
    "        ['gdal_contour', '-a', 'elevation', '-i', str(contourInterval), '-f', 'ESRI Shapefile', pathToDEMFile,\n",
    "         outputPath], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    return print('Contour shapefile is created ... OK!')\n",
    "\n",
    "\n",
    "# Create slope from the given DEM. For this function to work 'gdal' libraries needs to be installed.\n",
    "def createSlope(pathToDEMFile, outputPath):\n",
    "    subprocess.call(['gdaldem', 'slope', pathToDEMFile, outputPath, '-of', 'GTiff', '-compute_edges'],\n",
    "                    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    return print('Slope raster is created ... OK!')\n",
    "\n",
    "\n",
    "# Get the exterior coordinates of the glacier outline shapefile.\n",
    "def getPolygonExteriorCoords(fionaFeature):\n",
    "    \"\"\"\n",
    "    输入：fiona feature\n",
    "    输出：所有 Polygon / MultiPolygon 的外边界 LineString 列表\n",
    "    \"\"\"\n",
    "    polygon = shape(fionaFeature[\"geometry\"])\n",
    "    exterior_lines = []\n",
    "\n",
    "    if polygon.geom_type == \"Polygon\":\n",
    "        # 单 Polygon\n",
    "        exterior_lines.append(LineString(polygon.exterior.coords))\n",
    "\n",
    "    elif polygon.geom_type == \"MultiPolygon\":\n",
    "        # MultiPolygon 必须使用 polygon.geoms 来迭代\n",
    "        for subpoly in polygon.geoms:\n",
    "            exterior_lines.append(LineString(subpoly.exterior.coords))\n",
    "\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported geometry type: {polygon.geom_type}\")\n",
    "\n",
    "    return exterior_lines\n",
    "# Get the interior coordinates of the glacier outline shapefile.\n",
    "# This is needed in cases where the glacier has internal rings inside the boundary. Compare 'ponkar' and 'mera' glacier\n",
    "# to see the differences\n",
    "# def getPolygonInteriorCoords(fionaFeature):\n",
    "#     interiorRings = []\n",
    "#     polygon = shape(fionaFeature['geometry'])\n",
    "#     if polygon.geom_type == 'Polygon':\n",
    "#         for interiorCoords in polygon.interiors:\n",
    "#             interiorRings.append(LineString(interiorCoords))\n",
    "#     elif polygon.geom_type == 'MultiPolygon':\n",
    "#         for individualPolygons in polygon:\n",
    "#             for interiorCoords in individualPolygons.interiors:\n",
    "#                 interiorRings.append(LineString(interiorCoords))\n",
    "#     else:\n",
    "#         print('WARNING: at function \"getPolygonInteriorCoords\", supported geometry not found!! Returning empty list.')\n",
    "#         pass\n",
    "#     return interiorRings\n",
    "\n",
    "# def getPolygonInteriorCoords(fionaFeature):\n",
    "#     \"\"\"\n",
    "#     输出 MultiPolygon 或 Polygon 的所有内环（洞）\n",
    "#     \"\"\"\n",
    "#     polygon = shape(fionaFeature[\"geometry\"])\n",
    "#     interior_lines = []\n",
    "\n",
    "#     if polygon.geom_type == \"Polygon\":\n",
    "#         for ring in polygon.interiors:\n",
    "#             interior_lines.append(LineString(ring.coords))\n",
    "\n",
    "#     elif polygon.geom_type == \"MultiPolygon\":\n",
    "#         for subpoly in polygon.geoms:\n",
    "#             for ring in subpoly.interiors:\n",
    "#                 interior_lines.append(LineString(ring.coords))\n",
    "\n",
    "#     else:\n",
    "#         raise TypeError(f\"Unsupported geometry type: {polygon.geom_type}\")\n",
    "\n",
    "#     return interior_lines\n",
    "\n",
    "\n",
    "def getPolygonExteriorCoords(fionaFeature):\n",
    "    polygon = shape(fionaFeature[\"geometry\"])\n",
    "    polygonExteriorCoords = []\n",
    "\n",
    "    # 单个Polygon\n",
    "    if polygon.geom_type == 'Polygon':\n",
    "        polygonExteriorCoords.append(LineString(list(polygon.exterior.coords)))\n",
    "\n",
    "    # MultiPolygon：必须使用 polygon.geoms 遍历\n",
    "    elif polygon.geom_type == 'MultiPolygon':\n",
    "        for poly in polygon.geoms:\n",
    "            polygonExteriorCoords.append(LineString(list(poly.exterior.coords)))\n",
    "\n",
    "    return polygonExteriorCoords\n",
    "\n",
    "\n",
    "\n",
    "def getPolygonInteriorCoords(feature):\n",
    "    \"\"\"\n",
    "    返回每个 polygon 的内环（holes）作为 LineString 列表\n",
    "    支持 Fiona feature dict / shapely Polygon / MultiPolygon / GeometryCollection\n",
    "    \"\"\"\n",
    "    # 1) 处理 fiona feature\n",
    "    if isinstance(feature, dict) and \"geometry\" in feature:\n",
    "        geom_json = feature.get(\"geometry\")\n",
    "        if geom_json is None:\n",
    "            return []\n",
    "        try:\n",
    "            geom = shape(geom_json)\n",
    "        except Exception:\n",
    "            return []\n",
    "    else:\n",
    "        geom = feature\n",
    "\n",
    "    if not hasattr(geom, \"geom_type\"):\n",
    "        return []\n",
    "\n",
    "    interior_lines = []\n",
    "\n",
    "    if geom.geom_type == \"Polygon\":\n",
    "        for ring in geom.interiors:\n",
    "            interior_lines.append(LineString(list(ring.coords)))\n",
    "    elif geom.geom_type == \"MultiPolygon\":\n",
    "        for subpoly in geom.geoms:\n",
    "            for ring in subpoly.interiors:\n",
    "                interior_lines.append(LineString(list(ring.coords)))\n",
    "    elif geom.geom_type == \"GeometryCollection\":\n",
    "        for part in geom.geoms:\n",
    "            if part.geom_type == \"Polygon\":\n",
    "                for ring in part.interiors:\n",
    "                    interior_lines.append(LineString(list(ring.coords)))\n",
    "            elif part.geom_type == \"MultiPolygon\":\n",
    "                for subpoly in part.geoms:\n",
    "                    for ring in subpoly.interiors:\n",
    "                        interior_lines.append(LineString(list(ring.coords)))\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    return interior_lines\n",
    "\n",
    "\n",
    "# \n",
    "# Read the metadata of the given DEM. Includes all necessary information such as resolution, extents, etc.\n",
    "def getRasterMetadata(rasterFileFullPath):\n",
    "    with rasterio.open(rasterFileFullPath) as rasterFile:\n",
    "        rasterMetadata = rasterFile.meta.copy()\n",
    "    return rasterMetadata\n",
    "\n",
    "\n",
    "# Check if the raster file is in projected coordinate system. Model does not work for GCS projection.\n",
    "# TODO: Check shapefile as well.\n",
    "def checkRasterProjection(rasterFileFullPath):\n",
    "    rasterMetaData = getRasterMetadata(rasterFileFullPath)\n",
    "    rasterCRS = rasterMetaData['crs']\n",
    "    print('DEM is not in projected Coordinate system. Please make sure it is! Model will now exit ...') or exit() \\\n",
    "        if rasterCRS.is_projected is False else None\n",
    "    return print('Raster projected seem to be OK!')\n",
    "\n",
    "\n",
    "# Get cosine values when input is in degrees.\n",
    "def cosd(angleInDegrees):\n",
    "    cosD = math.cos(math.radians(angleInDegrees))\n",
    "    return cosD\n",
    "\n",
    "\n",
    "# Get sine values when input is in degrees.\n",
    "def sind(angleInDegrees):\n",
    "    sinD = math.sin(math.radians(angleInDegrees))\n",
    "    return sinD\n",
    "\n",
    "\n",
    "# Get the mean raster slope. This is used in the estimation of the basal stress.\n",
    "def getMeanRasterSlope(rasterioRasterObject, shapelyPolygonObject, returnMetaData=False):\n",
    "    rasterMetaData = rasterioRasterObject.meta.copy()\n",
    "    polygon = [mapping(shapelyPolygonObject)]\n",
    "    maskedRaster, maskedRasterAffine = mask(rasterioRasterObject, polygon, nodata=np.nan)\n",
    "    rasterMetaData.update({\n",
    "        'driver': 'GTiff',\n",
    "        'height': maskedRaster.shape[1],\n",
    "        'width': maskedRaster.shape[2],\n",
    "        'transform': maskedRasterAffine\n",
    "    })\n",
    "    meanRasterSlope = np.nanmean(maskedRaster)\n",
    "    if returnMetaData:\n",
    "        return meanRasterSlope, rasterMetaData\n",
    "    else:\n",
    "        return meanRasterSlope\n",
    "\n",
    "\n",
    "# The changing outline of the glacier.\n",
    "def createBufferedOutlines(inputShapefile, outputShapefile, rasterFileFullPath, bufferedist):\n",
    "    with fiona.open(inputShapefile) as shapefile:\n",
    "        driver = shapefile.driver\n",
    "        crs = shapefile.crs\n",
    "        schema = {'properties': {'ID': 'int', 'area': 'float', 'tau': 'float'}, 'geometry': 'MultiPolygon'}\n",
    "        errorMsg = 'Given shapefile of the glacier outline contains more than 1 feature, ' \\\n",
    "                   'please ensure that the file has only one geometry \\nModel will now exit...'\n",
    "        print(errorMsg) or exit() if shapefile.__len__() != 1 else None\n",
    "        with fiona.open(outputShapefile, 'w', crs=crs, schema=schema, driver=driver) as output:\n",
    "            with rasterio.open(rasterFileFullPath) as raster:\n",
    "                for features in shapefile:\n",
    "                    outlineshape = shape(features['geometry'])\n",
    "                    unionedpoly = unary_union([outlineshape])\n",
    "                    initialArea = outlineshape.area\n",
    "                    fid = 0\n",
    "                    buffered = outlineshape.buffer(bufferedist)\n",
    "                    while buffered.geom_type != 'GeometryCollection':\n",
    "                    # while unary_union([outlineshape.buffer(bufferedist)]).type != 'GeometryCollection':\n",
    "                        # 计算tau\n",
    "                        tau = (3.0e4) * (((initialArea - unionedpoly.area) /\n",
    "                                        cosd(getMeanRasterSlope(raster, unionedpoly))) ** 0.106)\n",
    "\n",
    "                        # 输出统一为 MultiPolygon\n",
    "                        geom_to_write = unionedpoly\n",
    "                        if geom_to_write.geom_type == 'Polygon':\n",
    "                            geom_to_write = MultiPolygon([geom_to_write])\n",
    "\n",
    "                        output.write({\n",
    "                            'properties': {\n",
    "                                'ID': fid,\n",
    "                                'area': initialArea - unionedpoly.area,\n",
    "                                'tau': tau,\n",
    "                            },\n",
    "                            'geometry': mapping(geom_to_write)\n",
    "                        })\n",
    "\n",
    "                        fid += 1\n",
    "\n",
    "                        # 继续缓冲\n",
    "                        new_outline = outlineshape.buffer(bufferedist)\n",
    "                        if new_outline.is_empty:\n",
    "                            break\n",
    "\n",
    "                        unionedpoly = unary_union([new_outline])\n",
    "\n",
    "                        # 统一类型\n",
    "                        if unionedpoly.geom_type == 'Polygon':\n",
    "                            outlineshape = unionedpoly\n",
    "                        else:\n",
    "                            outlineshape = unionedpoly\n",
    "\n",
    "                        if unionedpoly.is_empty:\n",
    "                            break\n",
    "\n",
    "                        # tau = (2.7 * 10 ** 4) * (((initialArea - unionedpoly.area) /\n",
    "                        #                           cosd(getMeanRasterSlope(raster, unionedpoly))) ** 0.106)\n",
    "                        # output.write({\n",
    "                        #     'properties':\n",
    "                        #         {\n",
    "                        #             'ID': fid,\n",
    "                        #             'area': initialArea - unionedpoly.area,\n",
    "                        #             'tau': tau,\n",
    "                        #         },\n",
    "                        #     'geometry': mapping(unionedpoly)\n",
    "                        # })\n",
    "                        # unionedpoly = (unary_union([outlineshape.buffer(bufferedist)]))\n",
    "                        # outlineshape = outlineshape.buffer(bufferedist)\n",
    "                        # fid += 1\n",
    "    return print('Buffered outline is created ... OK!')\n",
    "\n",
    "\n",
    "# Pick raster values at given coordinates\n",
    "def sampleRaster(rasterioRasterObject, samplingCoordX, samplingCoordY, band=1):\n",
    "    rasterData = rasterioRasterObject.read(band)\n",
    "    indexX, indexY = rasterioRasterObject.index(samplingCoordX, samplingCoordY)\n",
    "    rasterValue = rasterData[indexX][indexY]\n",
    "    return rasterValue\n",
    "\n",
    "\n",
    "# Averaged slope at given coordinate. See description for more details.\n",
    "def averagedSlopeSample(rasterObject, samplingCoordinateX, samplingCoordinateY, first=5, second=10, third=20, band=1):\n",
    "    \"\"\"\n",
    "    Returns single averaged raster value at sampled point. Average is done at 3x3, 5x5 and 7x7 raster grid.\n",
    "\n",
    "    The averaging is done based on the following:\n",
    "        1. average a 7x7 grid if value is less than 'first'. Default: 5 (in degrees for slope)\n",
    "        2. average a 5x5 grid if value is less than 'second'. Default: 10 (in degrees for slope)\n",
    "        3. average a 3x3 grid if value is less than 'third'. Default: 20 (in degrees for slope)\n",
    "        4. do not average if value exceeds 'third'. ie use the exact slope value\n",
    "    The grid is defined in the 'grid3x3', 'grid5x5', and 'grid7x7' variables in the function itself. modifying these\n",
    "    variables will give average at that grid.\n",
    "    i.e grid3x3 = [2, 5] ==> will give values averaged at:\n",
    "                             2 * 2 + 1 by 2 * 5 + 1 grid\n",
    "                              i.e 5 x 11 grid.\n",
    "    Defaults:\n",
    "        grid3x3 = [1, 1] => implies 2 * 1 + 1,  2 * 1 + 1 = 3x3 grid.\n",
    "        grid5x5 = [2, 2] => implies 2 * 2 + 1,  2 * 3 + 1 = 5x5 grid.\n",
    "        grid7x7 = [3, 3] => implies 2 * 3 + 1,  2 * 3 + 1 = 7x7 grid.\n",
    "\n",
    "    :param rasterObject: rasterio raster object. open the slope raster with rasterio.\n",
    "    :param samplingCoordinateX: x-coordinate of the point from where the value of slope is required.\n",
    "    :param samplingCoordinateY: y-coordinate of the point from where the value of slope is required.\n",
    "    :param band: The band of raster. Default is 1.\n",
    "    :param first: Value less than for which averaging is done with a 7x7 grid\n",
    "    :param second: Value less than for which averaging is done with a 5x5 grid\n",
    "    :param third:\n",
    "    :returns: returns single averaged raster value at sampled point. Average is done at 3x3, 5x5 and 7x7 raster grid.\n",
    "    \"\"\"\n",
    "    sumSlope = 0\n",
    "    grid3x3 = [1, 1]\n",
    "    grid5x5 = [2, 2]\n",
    "    grid7x7 = [3, 3]\n",
    "    rasterBand = rasterObject.read(band)\n",
    "    indexX, indexY = rasterObject.index(samplingCoordinateX, samplingCoordinateY)\n",
    "    slopeValue = rasterBand[indexX][indexY]\n",
    "    if slopeValue <= first:\n",
    "        count = 0\n",
    "        for xIndices in range(indexX - (grid7x7[0]), indexX + (grid7x7[0] + 1)):\n",
    "            for yIndices in range(indexY - (grid7x7[1]), indexY + (grid7x7[1] + 1)):\n",
    "                if 0 <= xIndices <= rasterBand.shape[0] and 0 <= yIndices <= rasterBand.shape[1]:\n",
    "                    try:\n",
    "                        if rasterBand[xIndices][yIndices] == rasterObject.get_nodatavals() \\\n",
    "                                or rasterBand[xIndices][yIndices] == np.nan:\n",
    "                            pass\n",
    "                        else:\n",
    "                            count += 1\n",
    "                            sumSlope = sumSlope + np.nansum(rasterBand[xIndices][yIndices])\n",
    "                    except IndexError:\n",
    "                        print('WARNING: Index Exceeded at', samplingCoordinateX, samplingCoordinateY,\n",
    "                              'resulting index was ', indexX, indexY)\n",
    "                else:\n",
    "                    pass\n",
    "        try:\n",
    "            averageSlope = sumSlope / count\n",
    "        except ZeroDivisionError:\n",
    "            print('WARNING: No Slope data found for ', grid7x7[0], 'x', grid7x7[1],\n",
    "                  ' grid. Returning the exact slope at sampling points')\n",
    "            averageSlope = slopeValue.astype(np.float64)\n",
    "    elif slopeValue < second:\n",
    "        count = 0\n",
    "        for xIndices in range(indexX - (grid5x5[0]), indexX + (grid5x5[0] + 1)):\n",
    "            for yIndices in range(indexY - (grid5x5[1]), indexY + (grid5x5[1] + 1)):\n",
    "                if 0 <= xIndices <= rasterBand.shape[0] and 0 <= yIndices <= rasterBand.shape[1]:\n",
    "                    try:\n",
    "                        if rasterBand[xIndices][yIndices] == rasterObject.get_nodatavals() \\\n",
    "                                or rasterBand[xIndices][yIndices] == np.nan:\n",
    "                            pass\n",
    "                        else:\n",
    "                            count += 1\n",
    "                            sumSlope = sumSlope + np.nansum(rasterBand[xIndices][yIndices])\n",
    "                    except IndexError:\n",
    "                        print('WARNING: Index Exceeded at', samplingCoordinateX, samplingCoordinateY,\n",
    "                              'resulting index was ', indexX, indexY)\n",
    "                else:\n",
    "                    pass\n",
    "        try:\n",
    "            averageSlope = sumSlope / count\n",
    "        except ZeroDivisionError:\n",
    "            print('WARNING: No Slope data found for ', grid5x5[0], 'x', grid5x5[1],\n",
    "                  ' grid. Returning the exact slope at sampling points')\n",
    "            averageSlope = slopeValue.astype(np.float64)\n",
    "    elif slopeValue < third:\n",
    "        count = 0\n",
    "        for xIndices in range(indexX - (grid3x3[0]), indexX + (grid3x3[0] + 1)):\n",
    "            for yIndices in range(indexY - (grid3x3[1]), indexY + (grid3x3[1] + 1)):\n",
    "                if 0 <= xIndices < rasterBand.shape[0] and 0 <= yIndices < rasterBand.shape[1]:\n",
    "                    try:\n",
    "                        if rasterBand[xIndices][yIndices] == rasterObject.get_nodatavals() \\\n",
    "                                or rasterBand[xIndices][yIndices] == np.nan:\n",
    "                            pass\n",
    "                        else:\n",
    "                            count += 1\n",
    "                            sumSlope = sumSlope + np.nansum(rasterBand[xIndices][yIndices])\n",
    "                    except IndexError:\n",
    "                        print('WARNING: Index Exceeded at', samplingCoordinateX, samplingCoordinateY,\n",
    "                              'resulting index was ', indexX, indexY)\n",
    "                else:\n",
    "                    pass\n",
    "        try:\n",
    "            averageSlope = sumSlope / count\n",
    "        except ZeroDivisionError:\n",
    "            print('WARNING: No Slope data found for ', grid3x3[0], 'x', grid3x3[1],\n",
    "                  ' grid. Returning the exact slope at sampling points')\n",
    "            averageSlope = slopeValue.astype(np.float64)\n",
    "    else:\n",
    "        averageSlope = slopeValue.astype(np.float64)\n",
    "    return averageSlope\n",
    "\n",
    "\n",
    "def getSamplePoints(shapelyGeometryWithLineStrings1, shapelyGeometryWithLineStrings2):\n",
    "    x, y = [], []\n",
    "    for features in shapelyGeometryWithLineStrings1:\n",
    "        outlineShape = shape(features['geometry'])\n",
    "        for individualContours in shapelyGeometryWithLineStrings2:\n",
    "            contourShape = shape(individualContours['geometry'])\n",
    "            intersectionX, intersectionY = contourShape.intersection(outlineShape)\n",
    "            x.append(intersectionX) if intersectionX else None\n",
    "            y.append(intersectionY) if intersectionY else None\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# After sampling the thickness values at few points, the sampled points are interpolated to get a gridded data.\n",
    "# 'nnbathy' algorithm is used here.\n",
    "def interpolateUngriddedData(inputCSVFile, outputCSVFile, metaData):\n",
    "    gridWidth = metaData['width']\n",
    "    gridHeight = metaData['height']\n",
    "    xMin = metaData['transform'][2]\n",
    "    xMax = xMin + metaData['transform'][0] * gridWidth\n",
    "    yMax = metaData['transform'][5]\n",
    "    yMin = yMax + metaData['transform'][4] * gridHeight\n",
    "    cmd = str(gridWidth) + 'x' + str(gridHeight)\n",
    "    with open(outputCSVFile, 'w') as interpolatedCSV:\n",
    "        subprocess.call([nnPATH, '-i', inputCSVFile, '-n', cmd, '-x', str(xMax), str(xMin), '-y', str(yMax),\n",
    "                         str(yMin), '-W', '0'], stdout=interpolatedCSV)\n",
    "    return print('Interpolation is completed ... OK!')\n",
    "\n",
    "\n",
    "# Create a raster file from the gridded thickness data.\n",
    "def createRaster(griddedData, outputRasterFullPath, metaData):\n",
    "    data = np.loadtxt(griddedData)\n",
    "    metaData.update({'dtype': 'float64', 'nodata': np.nan})\n",
    "    pointThicknessValues = np.fliplr(np.reshape(data[:, 2], (metaData['height'], metaData['width'])))\n",
    "    with rasterio.open(outputRasterFullPath, 'w', **metaData) as thicknessObject:\n",
    "        thicknessObject.write(pointThicknessValues, 1)\n",
    "    return print('Thickness raster is created ... OK!')\n",
    "\n",
    "\n",
    "# Subtract the thickness raster from DEM to get Bed Topography.\n",
    "def subtractRasters(firstRasterFullPath, secondRasterFullPath, outputRasterFullPath, firstBand=1, secondBand=1):\n",
    "    with rasterio.open(firstRasterFullPath) as firstRaster:\n",
    "        with rasterio.open(secondRasterFullPath) as secondRaster:\n",
    "            rasterMetaData = firstRaster.meta.copy()\n",
    "            rasterMetaData.update({'dtype': 'float64', 'nodata': np.nan})\n",
    "            with rasterio.open(outputRasterFullPath, 'w', **rasterMetaData) as outputRaster:\n",
    "                firstRasterData = firstRaster.read(firstBand).astype('float32')\n",
    "                secondRasterData = secondRaster.read(secondBand).astype('float32')\n",
    "                firstRasterData[firstRasterData == firstRaster.get_nodatavals()] = np.nan\n",
    "                secondRasterData[secondRasterData == secondRaster.get_nodatavals()] = np.nan\n",
    "                outputData = firstRasterData - secondRasterData\n",
    "                outputRaster.write(outputData, 1)\n",
    "    return print('Bed topograpgy is created ... OK!')\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Check if input files are ok.\n",
    "    checkFiles()\n",
    "\n",
    "    # Create output folder.\n",
    "    # cleanUp()\n",
    "\n",
    "    # To get the time taken by the model\n",
    "    startTime = time.perf_counter()\n",
    "\n",
    "    # Get raster meta data\n",
    "    rasterMetaData = getRasterMetadata(demFullPath)\n",
    "\n",
    "    # Check if the given raster is projected.\n",
    "    checkRasterProjection(demFullPath)\n",
    "\n",
    "    # Create contours.\n",
    "    createContour(demFullPath, contourFullPath, rasterMetaData['transform'][0])\n",
    "\n",
    "    # Create slope.\n",
    "    createSlope(demFullPath, slopeFullPath)\n",
    "\n",
    "    # Create buffered outlines\n",
    "    createBufferedOutlines(glacierOutlineFullPath, bufferedOutlineFullPath,\n",
    "                           slopeFullPath, rasterMetaData['transform'][4])\n",
    "\n",
    "    # Main section, where the magic happens\n",
    "    with fiona.open(bufferedOutlineFullPath) as bufferedPolygons, \\\n",
    "        fiona.open(contourFullPath) as contours, \\\n",
    "        rasterio.open(slopeFullPath) as slopeRaster, \\\n",
    "        open(sampledCSV, 'w') as ungriddedCSV, \\\n",
    "        fiona.open(glacierOutlineFullPath) as glacierOutline:\n",
    "            writeCSV = csv.writer(ungriddedCSV, delimiter=' ')\n",
    "\n",
    "            # Set the ice thickness at glacier boundary to zero.\n",
    "            for features in glacierOutline:\n",
    "                featureGeom = shape(features['geometry'])\n",
    "                exteriorCoords = featureGeom.exterior.coords\n",
    "                for points in exteriorCoords:\n",
    "                    writeCSV.writerow([points[0], points[1], 0])\n",
    "                interiorGeom = featureGeom.interiors\n",
    "                for intFeat in interiorGeom:\n",
    "                    interiorCoords = intFeat.coords\n",
    "                    for points in interiorCoords:\n",
    "                        writeCSV.writerow([points[0], points[1], 0])\n",
    "\n",
    "            # Get ice thickness at various sampling points.\n",
    "            for individualFeature in bufferedPolygons:\n",
    "                externalLines = getPolygonExteriorCoords(individualFeature)\n",
    "                internalLines = getPolygonInteriorCoords(individualFeature)\n",
    "                for individualContour in contours:\n",
    "                    intersectionPoints = []\n",
    "                    singleContour = shape(individualContour['geometry'])\n",
    "                    for individualLines in externalLines:\n",
    "                        intersectionPoints.append(singleContour.intersection(individualLines))\n",
    "                    if not internalLines:\n",
    "                        for individualLines in internalLines:\n",
    "                            intersectionPoints.append(singleContour.intersection(individualLines))\n",
    "                    for points in intersectionPoints:\n",
    "                        if points.is_empty is False:\n",
    "                            if points.geom_type == 'Point':\n",
    "                                slope = averagedSlopeSample(slopeRaster, points.xy[0][0], points.xy[1][0])\n",
    "                                h = individualFeature['properties']['tau']/(density * f * sind(slope) * g)\n",
    "                                writeCSV.writerow([points.xy[0][0], points.xy[1][0], h])\n",
    "                            elif points.geom_type == 'MultiPoint':\n",
    "                                # for individualPoints in points.geoms:\n",
    "                                #     slope = averagedSlopeSample(slopeRaster, individualPoints.xy[0][0],\n",
    "                                #                                 individualPoints.xy[1][0])\n",
    "                                #     h = individualFeature['properties']['tau'] / (density * f * sind(slope) * g)\n",
    "                                # 统一处理单点和多点（再也不会报错了）\n",
    "                                # —— 处理点（单点或多点）——\n",
    "                                point_list = [points] if points.geom_type == 'Point' else list(points.geoms)\n",
    "\n",
    "                                for pt in point_list:  # 用 pt 这个短名，避免混淆\n",
    "                                    x = pt.x          # 推荐用 .x 和 .y，更简单可靠\n",
    "                                    y = pt.y\n",
    "                                    # x = pt.xy[0][0]   # 旧写法也可以，但 .x/.y 更快\n",
    "                                    # y = pt.xy[1][0]\n",
    "\n",
    "                                    slope = averagedSlopeSample(slopeRaster, x, y)\n",
    "                                    h = individualFeature['properties']['tau'] / (density * f * sind(slope) * g)\n",
    "\n",
    "                                    # 只写一行就够了（你原来写了两次，这里统一）\n",
    "                                    writeCSV.writerow([x, y, h])\n",
    "                                # point_list = [points] if points.geom_type == 'Point' else list(points.geoms)\n",
    "                                # for individualPoint in point_list:\n",
    "                                #     x = individualPoint.xy[0][0]\n",
    "                                #     y = individualPoint.xy[1][0]\n",
    "                                #     slope = averagedSlopeSample(slopeRaster, x, y)\n",
    "                                #     h = individualFeature['properties']['tau'] / (density * f * sind(slope) * g)\n",
    "                                #     writeCSV.writerow([x, y, h])\n",
    "                                #     writeCSV.writerow([individualPoint.xy[0][0], individualPoint.xy[1][0], h])\n",
    "                            else:\n",
    "                                print('WARNING: Neither \"Point\" or \"MultiPoint\". Doing Nothing!!')\n",
    "                                pass\n",
    "    print('Sampling is completed ... OK!')\n",
    "\n",
    "    # Interpolate the un-gridded data\n",
    "    interpolateUngriddedData(sampledCSV, finalCSV, rasterMetaData)\n",
    "\n",
    "    # Create thickness raster.\n",
    "    createRaster(finalCSV, thicknessRaster, rasterMetaData)\n",
    "\n",
    "    # Create Bed Topography\n",
    "    subtractRasters(demFullPath, thicknessRaster, bedTopoFullPath)\n",
    "\n",
    "    # Total time taken by the model\n",
    "    print('\\nTotal time elapsed = ', time.perf_counter() - startTime, ' seconds')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "OGGM GlabTop 冰川厚度生成脚本\n",
    "输入：冰川 RGI ID\n",
    "输出：ice_thickness.tif, bed_dem.tif\n",
    "\"\"\"\n",
    "from oggm import cfg, utils, workflow, graphics, tasks, DEFAULT_BASE_URL\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from oggm import cfg, utils, workflow, tasks\n",
    "from oggm.core import gis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import oggm\n",
    "import os\n",
    "from oggm.core import inversion\n",
    "import salem\n",
    "import glob\n",
    "import xarray as xr\n",
    "import rioxarray as rioxr\n",
    "# -----------------------------\n",
    "# 1. 配置工作目录和 OGGM 参数\n",
    "# -----------------------------\n",
    "cfg.initialize(logging_level='WARNING')\n",
    "cfg.PARAMS['use_multiprocessing'] = True\n",
    "cfg.PARAMS['mp_processes'] = 12\n",
    "# 初始化\n",
    "cfg.PATHS['working_dir'] = '/mnt/d/area/glims/glenglat_asia'\n",
    "cfg.PARAMS['use_intersects'] = False\n",
    "# rgi_ids = [\"RGI60-13.48434\"]\n",
    "rgi_ids = [\"RGI60-13.53249\"]\n",
    "'''\n",
    "RGI60-13.53249\n",
    "RGI60-13.53249\n",
    "RGI60-15.09026\n",
    "RGI60-13.54211\n",
    "RGI60-13.49367\n",
    "RGI60-13.54364\n",
    "RGI60-13.53222\n",
    "\n",
    "\n",
    "'''\n",
    "# Geometrical centerline\n",
    "# Where to store the data \n",
    "\n",
    "# We start from prepro level 3 with all data ready - note the url here\n",
    "#base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.3/centerlines/W5E5/'\n",
    "base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2025.6/elev_bands/ERA5/per_glacier_spinup/'\n",
    "#base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/RGI70G/b_080/'\n",
    "gdirs = workflow.init_glacier_directories(rgi_ids, from_prepro_level=3,prepro_border=160,prepro_base_url=base_url)\n",
    "\n",
    "# to assess the model geometry (for e.g. distributed ice thickness plots),\n",
    "# we need to set that to true\n",
    "#cfg.PARAMS['store_model_geometry'] = True\n",
    "#workflow.execute_entity_task(tasks.prepare_for_inversion, gdirs)\n",
    "\n",
    "# Default parameters\n",
    "# Deformation: from Cuffey and Patterson 2010\n",
    "glen_a = 2.4e-24\n",
    "# Sliding: from Oerlemans 1997\n",
    "fs = 5.7e-20\n",
    "# Distribute\n",
    "workflow.execute_entity_task(tasks.distribute_thickness_per_altitude, gdirs);\n",
    "# xarray is an awesome library! Did you know about it?\n",
    "\n",
    "ds = xr.open_dataset(gdirs[0].get_filepath('gridded_data'))\n",
    "ds.distributed_thickness.plot();\n",
    "# save the distributed ice thickness into a geotiff file\n",
    "workflow.execute_entity_task(tasks.gridded_data_var_to_geotiff, gdirs, varname='distributed_thickness')\n",
    "\n",
    "# The default path of the geotiff file is in the glacier directory with the name \"distributed_thickness.tif\"\n",
    "# Let's check if the file exists\n",
    "for gdir in gdirs:\n",
    "    path = os.path.join(gdir.dir, \"distributed_thickness.tif\")\n",
    "    assert os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3097ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "OGGM GlabTop 冰川厚度生成脚本\n",
    "输入：冰川 RGI ID\n",
    "输出：ice_thickness.tif, bed_dem.tif\n",
    "\"\"\"\n",
    "from oggm import cfg, utils, workflow, graphics, tasks, DEFAULT_BASE_URL\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from oggm.core import gis,inversion\n",
    "import pandas as pd\n",
    "import oggm,os,salem,glob\n",
    "import rioxarray as rioxr\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 配置工作目录和 OGGM 参数\n",
    "# -----------------------------\n",
    "cfg.initialize(logging_level='WARNING')\n",
    "cfg.PARAMS['use_multiprocessing'] = True\n",
    "cfg.PARAMS['mp_processes'] = 12\n",
    "# 初始化\n",
    "cfg.PATHS['working_dir'] = '/mnt/d/area/glims/glenglat_asia'\n",
    "cfg.PARAMS['use_intersects'] = False\n",
    "# rgi_ids = [\"RGI60-13.48434\"]\n",
    "\n",
    "rgi_list=['RGI60-13.53249','RGI60-15.09026','RGI60-13.54211','RGI60-13.49367','RGI60-13.54364','RGI60-13.53222']\n",
    "for i in range(len(rgi_list)):\n",
    "    rgi_ids = [str(rgi_list[i])]\n",
    "    # Geometrical centerline\n",
    "    # Where to store the data \n",
    "\n",
    "    # We start from prepro level 3 with all data ready - note the url here\n",
    "    base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.3/centerlines/W5E5/'\n",
    "    #base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2025.6/elev_bands/ERA5/per_glacier_spinup/'\n",
    "    #base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/RGI70G/b_080/'\n",
    "    gdirs = workflow.init_glacier_directories(rgi_ids, from_prepro_level=3,prepro_border=160,prepro_base_url=base_url)\n",
    "\n",
    "    # to assess the model geometry (for e.g. distributed ice thickness plots),\n",
    "    # we need to set that to true\n",
    "    #cfg.PARAMS['store_model_geometry'] = True\n",
    "    #workflow.execute_entity_task(tasks.prepare_for_inversion, gdirs)\n",
    "\n",
    "    # Default parameters\n",
    "    # Deformation: from Cuffey and Patterson 2010\n",
    "    glen_a = 2.4e-24\n",
    "    # Sliding: from Oerlemans 1997\n",
    "    fs = 5.7e-20\n",
    "    # Distribute\n",
    "    workflow.execute_entity_task(tasks.distribute_thickness_per_altitude, gdirs);\n",
    "    # xarray is an awesome library! Did you know about it?\n",
    "\n",
    "    ds = xr.open_dataset(gdirs[0].get_filepath('gridded_data'))\n",
    "    # \n",
    "    # save the distributed ice thickness into a geotiff file\n",
    "    workflow.execute_entity_task(tasks.gridded_data_var_to_geotiff, gdirs, varname='distributed_thickness')\n",
    "    ds.distributed_thickness.plot();\n",
    "    # The default path of the geotiff file is in the glacier directory with the name \"distributed_thickness.tif\"\n",
    "    # Let's check if the file exists\n",
    "    for gdir in gdirs:\n",
    "        path = os.path.join(gdir.dir, \"distributed_thickness.tif\")\n",
    "        assert os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d35c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "OGGM GlabTop 冰川厚度生成脚本\n",
    "输入：冰川 RGI ID\n",
    "输出：ice_thickness.tif, bed_dem.tif\n",
    "\"\"\"\n",
    "from oggm import cfg, utils, workflow, graphics, tasks, DEFAULT_BASE_URL\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from oggm.core import gis,inversion\n",
    "import pandas as pd\n",
    "import oggm,os,salem,glob,shutil\n",
    "import rioxarray as rioxr\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 配置工作目录和 OGGM 参数\n",
    "# -----------------------------\n",
    "cfg.initialize(logging_level='WARNING')\n",
    "cfg.PARAMS['use_multiprocessing'] = True\n",
    "cfg.PARAMS['mp_processes'] = 12\n",
    "# 初始化\n",
    "cfg.PATHS['working_dir'] = '/mnt/d/area/glims/glenglat_asia'\n",
    "cfg.PARAMS['use_intersects'] = False\n",
    "# rgi_ids = [\"RGI60-13.48434\"]\n",
    "\n",
    "rgi_list=['RGI60-13.53249','RGI60-15.09026','RGI60-13.54211','RGI60-13.49367','RGI60-13.54364','RGI60-13.53222']\n",
    "for i in range(len(rgi_list)):\n",
    "\n",
    "    # Geometrical centerline\n",
    "    # Where to store the data \n",
    "\n",
    "    # We start from prepro level 3 with all data ready - note the url here\n",
    "    base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.3/centerlines/W5E5/'\n",
    "    #base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2025.6/elev_bands/ERA5/per_glacier_spinup/'\n",
    "    #base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/RGI70G/b_080/'\n",
    "    rgi_ids = rgi_list[i]\n",
    "    gdirs = workflow.init_glacier_directories(rgi_ids, from_prepro_level=3,prepro_border=160,prepro_base_url=base_url)\n",
    "    gdir = gdirs[0]\n",
    "    rgi_ids = [str(rgi_list[i])]\n",
    "    srcpath = gdir.get_filepath('centerlines')\n",
    "    new_name = os.path.dirname(srcpath)+'/'+f\"{gdir.rgi_id}_centerline.shp\"\n",
    "    utils.write_centerlines_to_shape(gdirs,  # The glaciers to process\n",
    "                                    path=new_name,  # The output file\n",
    "                                    ensure_exterior_match=True,\n",
    "                                    to_tar=False,  # set to True to put everything into one single tar file\n",
    "                                    simplify_line_before=0.75,# Write into the projection of the original inventory\n",
    "                                    simplify_line_after = 0.0,\n",
    "                                    keep_main_only=True,  # Write only the main flowline and discard the tributaries\n",
    "                                    corner_cutting=3\n",
    "                                    )\n",
    "\n",
    "    print(\"中心线文件：\", new_name)\n",
    "    cls_smooth = gpd.read_file(new_name)\n",
    "    graphics.plot_centerlines(gdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "OGGM GlabTop 冰川中心线生成点\n",
    "输入：冰川 RGI ID\n",
    "输出：\n",
    "\"\"\"\n",
    "from oggm import cfg, utils, workflow, graphics, tasks, DEFAULT_BASE_URL\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from oggm.core import gis,inversion\n",
    "import pandas as pd\n",
    "import oggm,os,salem,glob,shutil\n",
    "import rioxarray as rioxr\n",
    "from shapely.geometry import Point\n",
    "# -----------------------------\n",
    "# 1. 配置工作目录和 OGGM 参数\n",
    "# -----------------------------\n",
    "cfg.initialize(logging_level='WARNING')\n",
    "cfg.PARAMS['use_multiprocessing'] = True\n",
    "cfg.PARAMS['mp_processes'] = 12\n",
    "# 初始化\n",
    "cfg.PATHS['working_dir'] = '/mnt/d/area/glims/glenglat_asia'\n",
    "cfg.PARAMS['use_intersects'] = False\n",
    "# rgi_ids = [\"RGI60-13.48434\"]\n",
    "\n",
    "rgi_list=['RGI60-13.53249','RGI60-15.09026','RGI60-13.54211','RGI60-13.49367','RGI60-13.54364','RGI60-13.53222']\n",
    "for i in range(len(rgi_list)):\n",
    "\n",
    "    # Geometrical centerline\n",
    "    # Where to store the data \n",
    "\n",
    "    # We start from prepro level 3 with all data ready - note the url here\n",
    "    base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.3/centerlines/W5E5/'\n",
    "    #base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2025.6/elev_bands/ERA5/per_glacier_spinup/'\n",
    "    #base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/RGI70G/b_080/'\n",
    "    gdirs = workflow.init_glacier_directories(rgi_ids, from_prepro_level=3,prepro_border=160,prepro_base_url=base_url)\n",
    "    gdir = gdirs[0]\n",
    "    rgi_ids = [str(rgi_list[i])]\n",
    "    srcpath = gdir.get_filepath('centerlines')\n",
    "    new_name = os.path.dirname(srcpath)+'/'+f\"{gdir.rgi_id}_centerline.shp\"\n",
    "    utils.write_centerlines_to_shape(gdirs,  # The glaciers to process\n",
    "                                    path=new_name,  # The output file\n",
    "                                    ensure_exterior_match=True,\n",
    "                                    to_tar=False,  # set to True to put everything into one single tar file\n",
    "                                    simplify_line_before=0.75,# Write into the projection of the original inventory\n",
    "                                    simplify_line_after = 0.0,\n",
    "                                    keep_main_only=True,  # Write only the main flowline and discard the tributaries\n",
    "                                    corner_cutting=3\n",
    "                                    )\n",
    "\n",
    "    print(\"中心线文件：\", new_name)\n",
    "    # --- Step 1: 读取中心线 ---\n",
    "    cl = gpd.read_file(new_name)\n",
    "\n",
    "    # --- Step 2: 设置采样间距 ---\n",
    "    spacing = 30.0  # 每 30 m 一个采样点\n",
    "\n",
    "    points_list = []\n",
    "    # --- Step 3: 对每条中心线按固定距离采样 ---\n",
    "    for idx, row in cl.iterrows():\n",
    "        line = row.geometry  # LineString\n",
    "\n",
    "        # 计算线长\n",
    "        length = int(cl['LE_SEGMENT'])\n",
    "       \n",
    "        # 生成采样位置（0, 30, 60, ..., length）\n",
    "        distances = np.arange(0, length, spacing)\n",
    "\n",
    "        # 对每个距离进行采样\n",
    "        for i, d in enumerate(distances):\n",
    "            p = line.interpolate(d)  # 获取点\n",
    "            points_list.append({\n",
    "                \"cl_id\": idx,   # 第几条中心线\n",
    "                \"pt_id\": i,     # 点序号\n",
    "                \"dist\": float(d),  # 距离起点的长度（m）\n",
    "                \"geometry\": Point(p.x, p.y)\n",
    "            })\n",
    "        # --- Step 4: 转成 GeoDataFrame ---\n",
    "            gdf_points = gpd.GeoDataFrame(points_list, crs=cl.crs)\n",
    "\n",
    "    # --- Step 5: 导出 Shapefile ---\n",
    "    out_fp = os.path.join(gdir.dir, f\"{gdir.rgi_id}\"+\"_centerline_points_30m.shp\")\n",
    "    gdf_points.to_file(out_fp)\n",
    "\n",
    "    print(\"已导出 30 m 等距采样点：\", out_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca6441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "OGGM 主中心线 → 严格 30m 等距采样点（包含起点和终点）\n",
    "\"\"\"\n",
    "\n",
    "from oggm import cfg, utils, workflow\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# ==============================\n",
    "# 配置\n",
    "# ==============================\n",
    "cfg.initialize(logging_level='WORKFLOW')          # 改成 WORKFLOW 能看到更多信息\n",
    "cfg.PATHS['working_dir'] = '/mnt/d/area/glims/glenglat_asia'\n",
    "cfg.PARAMS['use_multiprocessing'] = False                 # 单冰川不用多进程\n",
    "cfg.PARAMS['border'] = 160\n",
    "\n",
    "# 你的冰川列表\n",
    "rgi_list = [\n",
    "    'RGI60-13.53249', 'RGI60-15.09026', 'RGI60-13.54211',\n",
    "    'RGI60-13.49367', 'RGI60-13.54364', 'RGI60-13.53222'\n",
    "]\n",
    "\n",
    "# OGGM 官方预处理中心线数据（level 3）\n",
    "base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.3/centerlines/W5E5/'\n",
    "\n",
    "# 采样间距（米）\n",
    "spacing = 30.0\n",
    "\n",
    "# ==============================\n",
    "# 主循环\n",
    "# ==============================\n",
    "for rgi_id in rgi_list:\n",
    "    print(f\"\\n正在处理: {rgi_id}\")\n",
    "\n",
    "    # 1. 初始化 glacier directory\n",
    "    gdirs = workflow.init_glacier_directories(\n",
    "        [rgi_id],\n",
    "        from_prepro_level=3,\n",
    "        prepro_border=160,\n",
    "        prepro_base_url=base_url\n",
    "    )\n",
    "    gdir = gdirs[0]\n",
    "\n",
    "    # 2. 导出主中心线（尽量少简化，防止线被砍成几米长）\n",
    "    centerline_shp = os.path.join(gdir.dir, f\"{gdir.rgi_id}_centerline.shp\")\n",
    "    utils.write_centerlines_to_shape(gdirs,  # The glaciers to process\n",
    "                                    path=centerline_shp,  # The output file\n",
    "                                    ensure_exterior_match=True,\n",
    "                                    to_tar=False,  # set to True to put everything into one single tar file\n",
    "                                    simplify_line_before=0.75,# Write into the projection of the original inventory\n",
    "                                    simplify_line_after = 0.0,\n",
    "                                    keep_main_only=True,  # Write only the main flowline and discard the tributaries\n",
    "                                    corner_cutting=3\n",
    "                                    )\n",
    "    print(f\"   中心线已导出 → {centerline_shp}\")\n",
    "\n",
    "    # 3. 读取中心线（通常只有一条主线）\n",
    "    lines_gdf = gpd.read_file(centerline_shp)   # ← 修改为你的路径\n",
    "\n",
    "    # 确保坐标系是投影坐标系（单位为米），常见的中国投影如 CGCS2000 / 3-degree Gauss-Kruger 或 UTM\n",
    "    # 如果原始数据是 WGS84（经纬度），请先投影到米为单位的坐标系\n",
    "    if lines_gdf.crs.is_geographic:\n",
    "        # 示例：转换为一个合适的投影坐标系（根据你的数据位置修改 EPSG）\n",
    "        lines_gdf = lines_gdf.to_crs(epsg=32650)   # 例如 UTM zone 50N（中国东部常用）\n",
    "\n",
    "    # 目标间隔（米）\n",
    "    distance = 30\n",
    "\n",
    "    # 用来存放所有生成点的列表\n",
    "    points_list = []\n",
    "    geom_type_list = []   # 可选：记录原始线的属性\n",
    "\n",
    "    for idx, line in lines_gdf.iterrows():\n",
    "        # 获取 shapely LineString 对象\n",
    "        geom = line.geometry\n",
    "        \n",
    "        # 如果是 MultiLineString，需要分别处理每一段\n",
    "        if geom.geom_type == 'MultiLineString':\n",
    "            for single_line in geom.geoms:\n",
    "                lengths = np.arange(0, single_line.length, distance)\n",
    "                # 在线的末端也生成一个点（如果恰好整除会重复，这里用 np.append 保证有终点）\n",
    "                lengths = np.append(lengths, single_line.length)\n",
    "                for d in lengths:\n",
    "                    point = single_line.interpolate(d)\n",
    "                    points_list.append(point)\n",
    "                    geom_type_list.append(idx)  # 保存原始线的索引或属性\n",
    "        else:  # LineString\n",
    "            lengths = np.arange(0, geom.length, distance)\n",
    "            lengths = np.append(lengths, geom.length)  # 保证终点也有点\n",
    "            for d in lengths:\n",
    "                point = geom.interpolate(d)\n",
    "                points_list.append(point)\n",
    "                geom_type_list.append(idx)\n",
    "\n",
    "    # 创建新的 GeoDataFrame（点）\n",
    "    points_gdf = gpd.GeoDataFrame(\n",
    "        geometry=points_list,\n",
    "        crs=lines_gdf.crs\n",
    "    )\n",
    "\n",
    "    # 如果想把原始线的属性带过来（推荐）\n",
    "    # 通过索引把原始属性合并进来\n",
    "    # points_gdf = points_gdf.join(lines_gdf.drop(columns='geometry'), on=geom_type_list)/\n",
    "\n",
    "    # 可选：给每个点增加一个距离字段（从起点算起的距离）\n",
    "    def calc_distance(row, lines_gdf):\n",
    "        orig_idx = row.name if isinstance(row.name, int) else geom_type_list[row.name]\n",
    "        line = lines_gdf.loc[orig_idx].geometry\n",
    "        if line.geom_type == 'MultiLineString':\n",
    "            # 简化处理：这里直接返回点到整条 MultiLineString 起点的距离\n",
    "            return line.project(row.geometry)\n",
    "        else:\n",
    "            return line.project(row.geometry)\n",
    "\n",
    "    # points_gdf['dist_along'] = points_gdf.apply(lambda row: calc_distance(row, lines_gdf), axis=1)\n",
    "\n",
    "    # 保存结果\n",
    "    out_fp = os.path.join(gdir.dir, f\"{gdir.rgi_id}\"+\"_centerline_points_30m.shp\")\n",
    "    points_gdf.to_file(out_fp)\n",
    "\n",
    "    print(f\"生成完成！共生成 {len(points_gdf)} 个点，已保存为 lines_points_30m.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd554aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "OGGM 主中心线 → 严格 30m 等距采样点（包含起点和终点）\n",
    "\"\"\"\n",
    "\n",
    "from oggm import cfg, utils, workflow,global_tasks,tasks\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# ==============================\n",
    "# 配置\n",
    "# ==============================\n",
    "cfg.initialize(logging_level='WORKFLOW')          # 改成 WORKFLOW 能看到更多信息\n",
    "cfg.PATHS['working_dir'] = '/mnt/d/area/glims/glenglat_asia'\n",
    "cfg.PARAMS['use_multiprocessing'] = False                 # 单冰川不用多进程\n",
    "cfg.PARAMS['border'] = 160\n",
    "cfg.PARAMS['evolution_model'] = 'FluxBased'\n",
    "# 你的冰川列表\n",
    "rgi_list = [\n",
    "    'RGI60-13.53249', 'RGI60-15.09026', 'RGI60-13.54211',\n",
    "    'RGI60-13.49367', 'RGI60-13.54364', 'RGI60-13.53222'\n",
    "]\n",
    "\n",
    "# OGGM 官方预处理中心线数据（level 3）\n",
    "base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.3/centerlines/W5E5/'\n",
    "\n",
    "# 采样间距（米）\n",
    "spacing = 30.0\n",
    "\n",
    "# ==============================\n",
    "# 主循环\n",
    "# ==============================\n",
    "for rgi_id in rgi_list:\n",
    "    print(f\"\\n正在处理: {rgi_id}\")\n",
    "\n",
    "    # 1. 初始化 glacier directory\n",
    "    gdirs = workflow.init_glacier_directories(\n",
    "        [rgi_id],\n",
    "        from_prepro_level=3,\n",
    "        prepro_border=160,\n",
    "        prepro_base_url=base_url\n",
    "    )\n",
    "    gdir = gdirs[0]\n",
    "    ELA=global_tasks.compile_ela(gdirs, ys=2000, ye=2019);\n",
    "    print(ELA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oggm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
